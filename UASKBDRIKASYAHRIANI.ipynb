{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8pYTdThYV8N",
        "outputId": "3af0afbc-e7c0-4be1-d5b6-7d05e20c88cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,HashingVectorizer\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "!pip install vaderSentiment\n",
        "import vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "eWZye5oKYiVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2510735c-fe23-4607-e3d5-f7969b54b55c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the data file\n",
        "#  read the data\n",
        "df1 = pd.read_csv(\"/content/gdrive/My Drive/UAS Data Mining/drugsComTrain_raw.csv\")\n",
        "df2 = pd.read_csv(\"/content/gdrive/My Drive/UAS Data Mining/drugsComTest_raw.csv\")\n",
        "# combine two file\n",
        "df = pd.concat([df1, df2])\n",
        "df\n",
        "# rename the cols\n",
        "df.columns = ['ID','drug name','condition','review','rating','date','useful count']"
      ],
      "metadata": {
        "id": "6ykgioTEYvRG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "4ff5c7f4-216e-4bbf-e498-ba73714a2042"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a507a6bb5290>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# preprocessing the data file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#  read the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/UAS Data Mining/drugsComTrain_raw.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive/My Drive/UAS Data Mining/drugsComTest_raw.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# combine two file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/UAS Data Mining/drugsComTrain_raw.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df[df['useful count'] > 10]"
      ],
      "metadata": {
        "id": "sElt5DUPY3CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_condition = df2.groupby(['condition'])['drug name'].nunique().sort_values(ascending=False)\n",
        "df_condition = pd.DataFrame(df_condition).reset_index()\n",
        "df_condition.tail(20)"
      ],
      "metadata": {
        "id": "a0cMZrbtY6ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_condition_1 = df_condition[df_condition['drug name'] == 1].reset_index()\n",
        "\n",
        "all_list = set(df.index)\n",
        "\n",
        "# deleting them\n",
        "condition_list = []\n",
        "for i,j in enumerate(df['condition']):\n",
        "    for c in list(df_condition_1['condition']):\n",
        "        if j == c:\n",
        "            condition_list.append(i)\n",
        "\n",
        "new_idx = all_list.difference(set(condition_list))\n",
        "df = df.iloc[list(new_idx)].reset_index()\n",
        "del df['index']"
      ],
      "metadata": {
        "id": "se9uzgceY9Rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "CYHYwlHTZAOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing the conditions with  in it.\n",
        "\n",
        "all_list = set(df.index)\n",
        "span_list = []\n",
        "for i,j in enumerate(df['condition']):\n",
        "    if \"\" in str(j):\n",
        "        span_list.append(i)\n",
        "new_idx = all_list.difference(set(span_list))\n",
        "df = df.iloc[list(new_idx)].reset_index()\n",
        "del df['index']"
      ],
      "metadata": {
        "id": "I0VRXJ29ZCyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "b0Vz219GZGSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing some stopwords from the list of stopwords as they are important for drug recommendation\n",
        "\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "not_stop = [\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\n",
        "            \"mustn't\",\"needn't\",\"no\",\"nor\",\"not\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"wouldn't\"]\n",
        "for i in not_stop:\n",
        "    stops.remove(i)"
      ],
      "metadata": {
        "id": "wxoq5rcyZKPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def review_to_words(raw_review):\n",
        "    # 1. Delete HTML\n",
        "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
        "    # 2. Make a space\n",
        "    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n",
        "    # 3. lower letters\n",
        "    words = letters_only.lower().split()\n",
        "    # 5. Stopwords\n",
        "    meaningful_words = [w for w in words if not w in stops]\n",
        "    # 6. Stemming\n",
        "    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n",
        "    # 7. space join words\n",
        "    return( ' '.join(stemming_words))\n",
        "\n",
        "# create a list of stopwords\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "parser = English()\n",
        "punctuations = string.punctuation\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "metadata": {
        "id": "7Lq2YNqLZNxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time df['review_clean'] = df['review'].apply(review_to_words)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "TjtpkL9zZRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,2))\n",
        "#  tf-idf vector\n",
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer)"
      ],
      "metadata": {
        "id": "2BwE8LK5ZXRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# part 1---vader sentiment analyzer for c_review\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "# create new col vaderReviewScore based on C-review\n",
        "df['vaderReviewScore'] = df['review_clean'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
        "\n",
        "# define the positive, neutral and negative\n",
        "positive_num = len(df[df['vaderReviewScore'] >=0.05])\n",
        "neutral_num = len(df[(df['vaderReviewScore'] >-0.05) & (df['vaderReviewScore']<0.05)])\n",
        "negative_num = len(df[df['vaderReviewScore']<=-0.05])\n",
        "\n",
        "# create new col vaderSentiment based on vaderReviewScore\n",
        "df['vaderSentiment'] = df['vaderReviewScore'].map(lambda x:int(2) if x>=0.05 else int(1) if x<=-0.05 else int(0) )\n",
        "df['vaderSentiment'].value_counts() # 2-pos: 99519; 1-neg: 104434; 0-neu: 11110\n",
        "\n",
        "# label pos/neg/neu based on vaderSentiment result\n",
        "df.loc[df['vaderReviewScore'] >=0.05,\"vaderSentimentLabel\"] =\"positive\"\n",
        "df.loc[(df['vaderReviewScore'] >-0.05) & (df['vaderReviewScore']<0.05),\"vaderSentimentLabel\"]= \"neutral\"\n",
        "df.loc[df['vaderReviewScore']<=-0.05,\"vaderSentimentLabel\"] = \"negative\""
      ],
      "metadata": {
        "id": "wIx_Q4tvZgCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['vaderReviewScore'].max()"
      ],
      "metadata": {
        "id": "iw-es32xZkSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['vaderReviewScore'].min()"
      ],
      "metadata": {
        "id": "6ebxXOagZnDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criteria = [df['vaderReviewScore'].between(-0.997, -0.799), df['vaderReviewScore'].between(-0.798, -0.601), df['vaderReviewScore'].between(-0.600, 0.403), df['vaderReviewScore'].between(-0.402, -0.205), df['vaderReviewScore'].between(-0.204, -0.007), df['vaderReviewScore'].between(-0.006,0.191), df['vaderReviewScore'].between(0.192, 0.389), df['vaderReviewScore'].between(0.390, 0.587), df['vaderReviewScore'].between(0.588, 0.785), df['vaderReviewScore'].between(0.786, 1)]\n",
        "values = [1, 2, 3,4,5,6,7,8,9,10]\n",
        "\n",
        "df['normalVaderScore'] = np.select(criteria, values, 0)"
      ],
      "metadata": {
        "id": "8IyNADbRZpOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "LrKMgFyMZrjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['meanNormalizedScore'] = (df['rating'] + df['normalVaderScore'])/2\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hRBu2jgBZs8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby(['condition','drug name', 'ID']).agg({'meanNormalizedScore' : ['mean']})\n",
        "grouped.to_csv('Medicare_Normalized_results')\n",
        "grouped1 = grouped.reset_index()\n",
        "grouped1.head(100)"
      ],
      "metadata": {
        "id": "b0vQPCkoZwIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped1.set_index('ID')"
      ],
      "metadata": {
        "id": "WRxxSzf5Zy9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ratings = grouped1.pivot_table(index = ['condition'], columns = ['ID'], values = 'meanNormalizedScore')\n",
        "user_ratings1 = grouped1.pivot_table(index = ['condition'], columns = ['ID'], values = 'meanNormalizedScore')\n",
        "user_ratings.head(20)\n",
        "user_ratings.to_csv('Itemtoitem_recom.csv')"
      ],
      "metadata": {
        "id": "nl8BfNZ7Z3F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_ratings.iloc[0,:].sum(axis=0)"
      ],
      "metadata": {
        "id": "19xqPX7PZ4BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's predict the rating for the first drug  for the 1st condition.\n",
        "#Therefore, we need to find the similarity between the them\n",
        "import math as np\n",
        "def takesec(num):\n",
        "    return num[1]\n",
        "\n",
        "cosine_vector = []\n",
        "num = 0\n",
        "l1 = 0\n",
        "l2 = 0\n",
        "for i in range(1,user_ratings.shape[0]):\n",
        "    num = 0\n",
        "    l1 = 0\n",
        "    l2 = 0\n",
        "    for j in range(user_ratings.shape[1]):\n",
        "        if not np.isnan(user_ratings.iloc[i,j]) and not np.isnan(user_ratings.iloc[0,j]):\n",
        "            num = num + (user_ratings.iloc[i,j] * user_ratings.iloc[0,j])\n",
        "        if not np.isnan(user_ratings.iloc[i,j]):\n",
        "            l1 = l1 + (user_ratings.iloc[i,j] * user_ratings.iloc[i,j])\n",
        "        if not np.isnan(user_ratings.iloc[0,j]):\n",
        "            l2 = l2 + (user_ratings.iloc[0,j] * user_ratings.iloc[0,j])\n",
        "    eventual_prod = np.sqrt(l1) * np.sqrt(l2)\n",
        "    if eventual_prod != 0 :\n",
        "        eventual_div = num/eventual_prod\n",
        "        cosine_vector.append([i,eventual_div])\n",
        "        cosine_vector.sort(key=takesec, reverse=True)"
      ],
      "metadata": {
        "id": "LjtKoKedZ6Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_vector[:100000]\n",
        "user_ratings1.iloc[4,1]"
      ],
      "metadata": {
        "id": "9BJisNsdZ9Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# opening the csv file in 'w+' mode\n",
        "file = open('cosine_vec.csv', 'w+', newline ='')\n",
        "\n",
        "# writing the data into the file\n",
        "with file:\n",
        "    write = csv.writer(file)\n",
        "    write.writerows(cosine_vector)"
      ],
      "metadata": {
        "id": "woFDrMGbaAI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's consider the top 50 similar rated drugs and predict the output.\n",
        "\n",
        "predict_drug = int(input (\"Enter a drug ID within range  0 to 1000000 : \"))\n",
        "count = 0\n",
        "num1 = 0\n",
        "den1 = 0\n",
        "for i in cosine_vector:\n",
        "    if user_ratings1.iloc[i[0],predict_drug] > 0:\n",
        "            count = count + 1\n",
        "            num1 = num1 + i[1] * user_ratings1.iloc[i[0],predict_drug]\n",
        "            #print(num1)\n",
        "            den1 = den1 + i[1]\n",
        "            #print(den1)\n",
        "            #print(\"{}..{}\".format(i[0],i[1]))\n",
        "\n",
        "    if count == 50:\n",
        "        print (\"Reached 50 :)\")\n",
        "        break\n",
        "\n",
        "print(\"Expected Rating for 1st Drug for condition: {} is :{}\".format(predict_drug,num1/den1))"
      ],
      "metadata": {
        "id": "_0mXzAjcaChS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(user_ratings1.shape[1]):\n",
        "    if user_ratings1.iloc[0,i] > 0:\n",
        "        print(\"Id = {} , Rating = {}\".format(i,user_ratings1.iloc[0,i]))"
      ],
      "metadata": {
        "id": "Z7sC_CuLaFbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}